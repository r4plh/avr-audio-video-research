# Q: A Systematic Framework for Reading ML Training Codebases

**Query:**

You can analyse how GOATs of this field like Andrej Karpathy and Umar Jamil and similar people go about this part, do all research, to finally give the answer.

I encounter many codebases, especially associated with a paper — like GPT-2, MusicGen, Whisper, DINet, wav2vec 2.0, HuBERT, GenHancer, HiFi-GAN, etc. I want to be proficient at reading training codebases. By this I mean: as soon as I enter a training codebase associated with a paper, or I myself let an AI generate training code, whatever code it generates, I can read it easily.

The main thing I have noticed is that training codes obviously have patterns — like there would be a training step involved, dataloaders (val and train), loss functions, data part, understanding the data part, data pipeline, augmentations, hyperparameters involved, what and how logging is being done, whether the logging of the val set is actually there to see and listen while the model is getting trained in proper logging, understanding of model architecture, seeing and analysing one batch which comes out of the dataloader to actually see whatever is going into the model is as per what we wanted — basically to check one batch to be sure there should not be misalignment in data. In what general sequence should one start the entry to a codebase and how to finish reading it in minimum possible time with 100% understanding from a research scientist's POV? Jupyter notebooks to understand some preprocessing steps or even notebooks to check intermediate outputs and steps of the model, strong test cases for important parts of the training pipeline — all these points are what I felt the importance of in my work. There may be more points which are important and could minimise the time given to a codebase and encourage 100% understanding, so those can be added too.

I need a guide/map/plan/template kind of thing which will make me skim through easily without confusion. I know what points to checkmark and understand move forward. Even though in a complex codebase the first iteration might not give 100%, I at least want a serial set of directions from start to end so that at least 60% understanding is done and I have reached the end. The missing black box which I missed, I can get after completing the first iteration, but completing the first iteration is necessary to start the second. I want things in an organised and documented manner.

Think in all directions to make this thing efficient — like any map, making an app which has checkmarks, a GitHub repo to maintain, whatever. Think in all directions, as this is one of the most core problems to solve for scientists: they want things to go in minimum time with maximum understanding, and further iterations to fill the loopholes from the previous iterations.

---

# A: A Systematic Framework for Reading Any ML Training Codebase

**No published guide exists for reading ML training codebases end-to-end — this is a documented gap in the community.** What follows synthesizes the actual practices of elite practitioners (Karpathy, Howard, Rush, Olah, Kilcher, Umar Jamil) with formal code-reading research and ML debugging checklists into a single repeatable framework. The core insight, drawn from Karpathy's famous "Recipe for Training Neural Networks" and Keshav's three-pass paper-reading method, is that **codebase understanding is not a single act but a structured, iterative process** — three passes of increasing depth, each with specific checkpoints and outputs. A research scientist applying this framework should reach ~60% understanding in 30–90 minutes (Pass 1), ~85% in a few hours (Pass 2), and near-complete understanding through active reproduction (Pass 3).

---

## The Universal Anatomy of ML Training Codebases

Before reading any codebase, it helps to know what you're looking for. Across hundreds of open-source ML projects — from nanoGPT to HiFi-GAN to wav2vec 2.0 — training codebases converge on a remarkably consistent architecture. Recognizing these **ten universal components** instantly gives you a mental map:

1. **Entry point** (`train.py` or `main.py`) — orchestrates everything; this is always where you start
2. **Configuration** — hyperparameters and experiment settings via Hydra/OmegaConf YAML files, argparse CLI flags, or JSON configs
3. **Data pipeline** — `Dataset` subclass with `__getitem__`/`__len__`, `DataLoader` wrappers, preprocessing transforms, train/val/test splits
4. **Model architecture** — `nn.Module` subclasses with `__init__` (layer definitions) and `forward` (computation graph)
5. **Loss function** — standard (`CrossEntropyLoss`, `MSELoss`) or custom loss modules
6. **Optimizer and scheduler** — Adam/AdamW/SGD instantiation plus learning rate scheduling (cosine annealing, warmup, etc.)
7. **Training loop** — the forward-backward cycle: `optimizer.zero_grad() → loss = model(x) → loss.backward() → optimizer.step()`
8. **Logging** — Weights & Biases, TensorBoard, or Neptune integration for tracking loss curves, metrics, and gradient norms
9. **Checkpointing** — saving/loading `model.state_dict()` and `optimizer.state_dict()`, typically best-validation and last-epoch checkpoints
10. **Evaluation and inference** — separate `eval.py` or `predict.py` scripts with `model.eval()` and `torch.no_grad()` contexts

The typical directory layout follows a predictable pattern: `configs/` for experiment YAML files, `src/` or a project-named folder containing `models/`, `data/`, `trainers/`, `losses/`, `metrics/`, and `utils/` subdirectories, plus top-level `train.py`, `eval.py`, `requirements.txt`, and a `README`. Templates like the **victoresque/pytorch-template** (5,000+ GitHub stars) and the **Lightning-Hydra template** codify this exact structure with abstract base classes. Knowing this skeleton means that even an unfamiliar codebase feels navigable — you are not discovering structure, you are confirming which variant of a known structure this project uses.

---

## Pass 1: Structural Reconnaissance (30–90 Minutes)

This first pass mirrors what Yannic Kilcher does when he first encounters a paper: **read the title, abstract, headings, and conclusions before engaging with content**. For code, the equivalent is reading metadata and structure without tracing any logic. Jeremy Howard's "whole game" philosophy applies here — understand what the codebase does and produces before understanding how.

**Start with the README and configuration files.** The README tells you the task, dataset, framework, and how to run training. Config files (YAML, JSON, or argparse defaults) reveal every controllable knob — learning rate, batch size, architecture choices, data paths, augmentation flags. Karpathy emphasizes that configs are the "API surface" of a training pipeline: they tell you what the authors considered important enough to make configurable.

**Map the file tree.** Run `tree -L 2` or use your IDE's explorer to catalog every file. Count lines of code with `cloc`. Identify the entry points (`train.py`, `eval.py`, `inference.py`). Flag the largest files — they usually contain the model definition or the training loop. A technique from the RSDW code-reading method (Nemil Dalal's influential HackerNoon framework): run `git log --pretty=format: --name-only | sort | uniq -c | sort -rg | head -10` to find the **most frequently modified files**, which are almost always the most important ones.

**Check dependencies.** Scan `requirements.txt` or `setup.py` for the deep learning framework (PyTorch, TensorFlow, JAX), experiment tracking (wandb, tensorboard), config management (hydra, omegaconf), and any domain-specific libraries (librosa for audio, torchvision for images). This instantly contextualizes the codebase.

**Skim the entry point without tracing.** Open `train.py` and read only the `main()` function or the top-level script flow. You are looking for the sequence: config loading → data setup → model instantiation → optimizer creation → training loop → checkpointing. Do not follow function calls yet. Just note the order and the names of the key functions/classes being invoked.

At the end of Pass 1, you should be able to answer five questions (adapted from Keshav's "5 C's"): What **category** of task is this (classification, generation, speech synthesis)? What **context** does it operate in (framework, libraries, hardware assumptions)? What is the **scale** (lines of code, number of modules, dataset size)? What appears **novel** compared to standard training pipelines? Is the code **well-organized** enough to justify a deeper read?

---

## Pass 2: Tracing Data Flow and Control Flow (2–4 Hours)

Pass 2 is where real understanding begins. Sasha Rush's annotated-implementation philosophy guides this stage: **map the paper's method section onto the code, line by line.** If you have the associated paper, keep it open side-by-side. Rush's "Annotated Transformer" demonstrated that the most effective way to understand a codebase is to align its structure with the paper's mathematical formulations.

**Trace the data pipeline first.** Karpathy's recipe begins with "become one with the data" — and for good reason. The data pipeline determines what the model actually sees, and subtle bugs here silently destroy performance. Trace the full path: raw data → preprocessing → `Dataset.__getitem__` → transforms/augmentation → `DataLoader` → batched tensors entering the model. Note the tensor shapes, dtypes, and value ranges at each stage. Umar Jamil's approach of decomposing a complex system into its constituent components works well here: treat the data pipeline as its own self-contained system to understand before moving to the model.

**Read the model architecture with the paper's figures.** Open the `nn.Module` definition. Read `__init__` to catalog all layers, then read `forward()` to understand the computation graph. Map each code block to the corresponding equation in the paper. Bold mental note: **attention mechanisms, normalization layers, skip connections, and positional encodings** are the components most likely to contain subtle, paper-specific implementations that deviate from standard library versions.

**Trace the training loop in detail.** Now follow one complete iteration: how a batch is fetched, fed through the model, how loss is computed, how gradients flow back, how optimizer steps happen, and what gets logged. Note whether the codebase uses gradient accumulation, mixed precision (`torch.cuda.amp`), gradient clipping, or distributed training (`DistributedDataParallel`). These details are rarely in the paper but critically affect reproducibility.

**Document as you go.** Chris Olah's "research debt" concept applies directly: the energy you invest in documenting your understanding now pays compound interest later. Create a single markdown file with three sections: (1) a box-and-arrow diagram of the data flow, (2) a table mapping paper sections/equations to specific code files and line numbers, and (3) a list of questions and surprises — things that don't match your expectations or the paper. Jeremy Howard insists that "exceptional people take a lot of notes — less exceptional people assume they're going to remember."

At the end of Pass 2, you should be able to explain the entire pipeline to a colleague: what data goes in, how it's transformed, what the model does, how it's trained, and what gets measured. You are at roughly **85% understanding**.

---

## Pass 3: Active Reproduction and Deep Verification

Pass 3 is Karpathy's domain. His entire recipe — overfit one batch, verify loss at initialization, visualize inputs just before the network, use backprop to chart dependencies — is a verification protocol that transforms passive reading into active understanding. This is also where Umar Jamil's "implement from scratch" philosophy shows its power: **you don't truly understand code until you've run it, broken it, and modified it.**

**Clone, install, and run.** Reproduce a training run, even if only for a few steps. Confirm the code actually executes. Note any undocumented dependencies, missing data files, or environment issues — these are common in research code.

**Apply Karpathy's sanity checks systematically.** These are the highest-value verification steps in all of ML engineering:

- **Fix the random seed** and confirm deterministic output across two identical runs
- **Verify loss at initialization** — for a classification task with *n* classes, initial loss should be approximately `-log(1/n)` (e.g., ~2.3 for 10-class softmax). If it's wildly different, something is wrong with the model initialization or loss computation
- **Overfit a single batch** — take 2–8 training examples, disable all regularization and augmentation, and train until loss reaches near zero. If the model cannot memorize a tiny dataset, there is a fundamental bug. This single check catches more bugs than any other technique
- **Visualize data just before it enters the model** — decode the raw tensors back into human-interpretable form (images, spectrograms, text) at the exact point of `model(x)`. This catches normalization errors, channel ordering bugs, and preprocessing mistakes that are invisible in tensor form
- **Use backprop to verify batch independence** — set the loss to the sum of outputs for a single example *i*, run `.backward()`, and verify that gradients are non-zero only for input *i*. This catches the insidious class of bugs where batch dimensions get accidentally mixed

**Step through with a debugger on one batch.** Set a breakpoint at the start of the training loop and step through one complete forward-backward pass. Inspect tensor shapes at every layer boundary. This is where IDE tools earn their keep: VSCode's debugger with the Python extension, or PyCharm's visual debugger, let you watch tensors transform through the network in real time.

**Write targeted tests.** Following the RSDW method's final step, write unit tests for the components you now understand: verify that the dataset returns correct shapes, that augmentations preserve label correctness, that the model's output shape matches expectations, that train/val/test splits don't overlap. These tests serve as both verification and documentation.

**Modify and observe.** Change a hyperparameter (learning rate, batch size) and confirm the effect matches your expectation. Swap a loss function. Remove a layer. This active experimentation, which Jeremy Howard calls "break things and see what happens," builds the kind of deep intuitive understanding that passive reading never achieves.

---

## Tools That Make the Process Faster

Research scientists navigating large codebases benefit enormously from the right tooling. **Claude Code** stands out as the most directly useful AI tool for codebase understanding: it operates in the terminal, indexes entire project structures, and can answer questions like "trace the data flow from raw audio to model input" or "what does the loss function in this repo actually compute?" by reading across files. It creates a `CLAUDE.md` project reference file and supports LSP-aware navigation in Python. For similar AI-assisted reading, **Cursor** offers semantic code search in a VSCode-like editor, and **GitHub Copilot** can explain unfamiliar code blocks inline.

For static analysis, **pydeps** generates visual dependency graphs of Python packages (run `pydeps src/` to see how modules connect), while **pyan** builds call graphs showing which functions call which. Both output SVG visualizations via Graphviz. The combination of `pydeps` for module-level architecture and `pyan` for function-level call chains gives you a visual map of any codebase in minutes.

IDE features worth configuring specifically for codebase reading include **VSCode Bookmarks** (mark key locations across files and jump between them), **GitLens** (see per-line git blame to understand why code was written a certain way), **Go to Definition** (F12) and **Find All References** (Shift+F12) for tracing symbol usage, and **breadcrumb navigation** for maintaining orientation in deeply nested files. In PyCharm, the **Hierarchy Viewer** for class inheritance and **Code Vision** annotations showing usage counts provide equivalent capabilities.

For personal documentation, maintain a markdown file per codebase using **Obsidian** or any plain-text editor. Structure it as: project overview, architecture diagram (even hand-drawn), paper-to-code mapping table, questions/surprises list, and modification log. This living document becomes invaluable when you return to the codebase weeks later.

---

## What the GOATs Teach Us About Mindset

Beyond specific techniques, the practitioners studied here converge on deeper principles that distinguish expert codebase readers from novices.

**Karpathy teaches patience and paranoia.** His foundational insight is that neural network training "fails silently" — unlike conventional software, a buggy training pipeline doesn't crash, it just produces subtly worse results. His quote captures the essential attitude: *"A 'fast and furious' approach to training neural networks does not work and only leads to suffering. The qualities that in my experience correlate most strongly to success in deep learning are patience and attention to detail."* He advocates being "thorough, defensive, paranoid, and obsessed with visualizations of basically every possible thing."

**Jeremy Howard teaches top-down boldness.** His "whole game" method, inspired by David Perkins, says: run the full pipeline first, get results, *then* understand why it works. Don't get stuck reading prerequisites — dive into the hard stuff and let confusion guide you to what you actually need to learn. His distilled advice: "Train lots of models."

**Chris Olah teaches that explanation IS understanding.** His "research debt" framework argues that if you can't explain a system clearly, you don't understand it well enough. The act of writing documentation, drawing diagrams, or teaching a concept to someone else is not a separate step from understanding — it is the final stage of understanding itself.

**Sasha Rush teaches that code IS explanation.** His annotated implementation approach — merging paper text, math notation, and working code into a single document — demonstrates that the gap between reading a paper and understanding its codebase can be closed by creating explicit, line-by-line mappings between the two.

**Umar Jamil teaches decomposition.** When facing a complex system like Stable Diffusion, he identifies every constituent paper and concept (DDPM, DDIM, U-Net, CLIP, VAE), understands each independently, then composes them. This divide-and-conquer approach applies directly to any multi-component training codebase.

---

## The Complete Codebase Reading Checklist

This checklist distills everything above into a single actionable reference. Print it, keep it in your notes app, or pin it next to your monitor.

### Pass 1 — Orientation (target: 60% understanding)

- [ ] Read README: task, dataset, framework, how to run
- [ ] Run `tree -L 2` and `cloc` to map structure and scale
- [ ] Read config files (YAML/JSON/argparse defaults) for all hyperparameters
- [ ] Check `requirements.txt` for framework and key libraries
- [ ] Skim entry point (`train.py`) for top-level flow without tracing calls
- [ ] Identify config management approach (Hydra, argparse, JSON)
- [ ] Identify logging approach (W&B, TensorBoard, custom)
- [ ] Find most-edited files via `git log` frequency analysis
- [ ] Answer the 5 C's: Category, Context, Scale, Novelty, Quality
- [ ] **Output:** One-paragraph summary of what this codebase does and how it's organized

### Pass 2 — Flow Tracing (target: 85% understanding)

- [ ] Trace data pipeline end-to-end: raw data → Dataset → DataLoader → model input
- [ ] Log tensor shapes, dtypes, and value ranges at each pipeline stage
- [ ] Read model architecture: `__init__` for layers, `forward()` for computation graph
- [ ] Map paper equations/figures to specific code files and line numbers
- [ ] Trace one full training iteration: batch fetch → forward → loss → backward → optimizer step
- [ ] Note gradient clipping, mixed precision, distributed training, accumulation steps
- [ ] Read evaluation loop and metrics computation
- [ ] Read checkpointing logic (what's saved, when, where)
- [ ] Draw a box-and-arrow data flow diagram
- [ ] Create paper-to-code mapping table
- [ ] List all questions, surprises, and deviations from the paper
- [ ] **Output:** Markdown doc with architecture diagram, paper-code map, and open questions

### Pass 3 — Active Verification (target: 95%+ understanding)

- [ ] Clone, install, and execute a training run (even a few steps)
- [ ] Fix random seed and verify deterministic output
- [ ] Verify loss at initialization matches `-log(1/n_classes)`
- [ ] Overfit a single batch of 2–8 examples to near-zero loss
- [ ] Visualize data exactly as it enters the model (decode tensors to human form)
- [ ] Use backprop dependency check: verify no batch-dimension mixing
- [ ] Step through one training iteration with a debugger, inspecting shapes at every layer
- [ ] Write unit tests for dataset shapes, augmentation correctness, split non-overlap
- [ ] Modify a hyperparameter and confirm expected effect
- [ ] Generate dependency graph with `pydeps` and/or call graph with `pyan`
- [ ] Resolve all open questions from Pass 2
- [ ] **Output:** Updated documentation, test suite, and confident ability to modify the codebase

---

## Conclusion

The framework above fills a genuine gap: despite the explosion of open-source ML training code, **no systematic guide for reading these codebases had been published** before this synthesis. The key insight unifying all expert approaches is that understanding is iterative and active, not linear and passive. Pass 1 gives you the map. Pass 2 lets you trace the territory. Pass 3 makes you a local. The single highest-leverage technique across all practitioners is Karpathy's "overfit one batch" — it catches more bugs per minute invested than any other verification step. The single most important habit is documentation: write down what you learn as you learn it, because as Jeremy Howard notes, the people who assume they'll remember are the ones who don't. Apply this framework to your next codebase — whether it's HuBERT, GenHancer, or DINet — and you will understand it faster, deeper, and more durably than through any ad hoc approach.
