{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "from transformers import WhisperModel, WhisperProcessor\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a53fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wa2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wa2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "whisper_model = WhisperModel.from_pretrained(\"openai/whisper-large-v3\")\n",
    "whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = ['yes', 'no', 'up', 'down', 'left']\n",
    "dataset_path = \"/data/aman/speech_commands/speech_commands_v0.02/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a385ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(audio_path, model_type=\"wav2vec\"):\n",
    "\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    if model_type == \"wav2vec\":\n",
    "        inputs = wa2vec_processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = wa2vec_model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    elif model_type == 'whisper':\n",
    "        inputs = whisper_processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = whisper_model.encoder(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deccd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "wav2vec_embeddings = []\n",
    "whisper_embeddings = []\n",
    "labels = []\n",
    "\n",
    "# Sample 50 files per word (to start)\n",
    "for word in tqdm(test_words, desc=\"Processing words\"):\n",
    "    word_dir = os.path.join(dataset_path, word)\n",
    "    files = os.listdir(word_dir)[:50]  # Just 50 samples per word initially\n",
    "    \n",
    "    for file in tqdm(files, desc=f\"Processing {word}\", leave=False):\n",
    "        if file.endswith('.wav'):\n",
    "            path = os.path.join(word_dir, file)\n",
    "            \n",
    "            # Get embeddings from both models\n",
    "            wav2vec_emb = extract_embeddings(path, 'wav2vec')\n",
    "            whisper_emb = extract_embeddings(path, 'whisper')\n",
    "            \n",
    "            wav2vec_embeddings.append(wav2vec_emb)\n",
    "            whisper_embeddings.append(whisper_emb)\n",
    "            labels.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wav2vec_embeddings = np.array(wav2vec_embeddings)\n",
    "whisper_embeddings = np.array(whisper_embeddings)\n",
    "\n",
    "# t-SNE for Wav2Vec\n",
    "wav2vec_tsne = TSNE(n_components=2, random_state=42,metric='cosine',max_iter=10000,n_iter_without_progress=1000)\n",
    "wav2vec_2d = wav2vec_tsne.fit_transform(wav2vec_embeddings)\n",
    "\n",
    "# t-SNE for Whisper  \n",
    "whisper_tsne = TSNE(n_components=2, random_state=42,metric='cosine',max_iter=10000,n_iter_without_progress=1000)\n",
    "whisper_2d = whisper_tsne.fit_transform(whisper_embeddings)\n",
    "\n",
    "# Create visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Color map for words\n",
    "colors = {'yes': 'red', 'no': 'blue', 'up': 'green', 'down': 'orange', 'left': 'purple'}\n",
    "\n",
    "# Plot Wav2Vec\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels]\n",
    "    ax1.scatter(wav2vec_2d[mask, 0], wav2vec_2d[mask, 1], \n",
    "                label=word, color=colors[word], alpha=0.6)\n",
    "ax1.set_title('Wav2Vec2 Embeddings')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot Whisper\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels]\n",
    "    ax2.scatter(whisper_2d[mask, 0], whisper_2d[mask, 1], \n",
    "                label=word, color=colors[word], alpha=0.6)\n",
    "ax2.set_title('Whisper Embeddings')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "wav2vec_embeddings = np.array(wav2vec_embeddings)\n",
    "whisper_embeddings = np.array(whisper_embeddings)\n",
    "\n",
    "# PCA for Wav2Vec with 3 components\n",
    "wav2vec_pca = PCA(n_components=3, random_state=42)\n",
    "wav2vec_3d = wav2vec_pca.fit_transform(wav2vec_embeddings)\n",
    "\n",
    "# PCA for Whisper with 3 components\n",
    "whisper_pca = PCA(n_components=3, random_state=42)\n",
    "whisper_3d = whisper_pca.fit_transform(whisper_embeddings)\n",
    "\n",
    "# Print variance explained\n",
    "print(f\"Wav2Vec PCA variance explained: {wav2vec_pca.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"Whisper PCA variance explained: {whisper_pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Create 3D visualizations\n",
    "fig = plt.figure(figsize=(16, 7))\n",
    "\n",
    "# Color map for words\n",
    "colors = {'yes': 'red', 'no': 'blue', 'up': 'green', 'down': 'orange', 'left': 'purple'}\n",
    "\n",
    "# 3D Plot for Wav2Vec\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels]\n",
    "    ax1.scatter(wav2vec_3d[mask, 0], wav2vec_3d[mask, 1], wav2vec_3d[mask, 2],\n",
    "                label=word, color=colors[word], alpha=0.6, s=50)\n",
    "ax1.set_title('Wav2Vec2 Embeddings (3D PCA)')\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_zlabel('PC3')\n",
    "ax1.legend()\n",
    "\n",
    "# 3D Plot for Whisper\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels]\n",
    "    ax2.scatter(whisper_3d[mask, 0], whisper_3d[mask, 1], whisper_3d[mask, 2],\n",
    "                label=word, color=colors[word], alpha=0.6, s=50)\n",
    "ax2.set_title('Whisper Embeddings (3D PCA)')\n",
    "ax2.set_xlabel('PC1')\n",
    "ax2.set_ylabel('PC2')\n",
    "ax2.set_zlabel('PC3')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# You can also rotate the view for better perspective\n",
    "# ax1.view_init(elev=20, azim=45)  # Adjust elevation and azimuth angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "wav2vec_embeddings = np.array(wav2vec_embeddings)\n",
    "whisper_embeddings = np.array(whisper_embeddings)\n",
    "\n",
    "# PCA for both models with 3 components\n",
    "wav2vec_pca = PCA(n_components=3, random_state=42)\n",
    "wav2vec_3d = wav2vec_pca.fit_transform(wav2vec_embeddings)\n",
    "\n",
    "whisper_pca = PCA(n_components=3, random_state=42)\n",
    "whisper_3d = whisper_pca.fit_transform(whisper_embeddings)\n",
    "\n",
    "# Print variance explained\n",
    "print(f\"Wav2Vec PCA variance explained: {wav2vec_pca.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"Whisper PCA variance explained: {whisper_pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Color map\n",
    "colors = {'yes': 'red', 'no': 'blue', 'up': 'green', 'down': 'orange', 'left': 'purple'}\n",
    "\n",
    "# Create subplots with 3D scatter\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
    "    subplot_titles=('Wav2Vec2 Embeddings (3D PCA)', 'Whisper Embeddings (3D PCA)')\n",
    ")\n",
    "\n",
    "# Add Wav2Vec traces\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=wav2vec_3d[mask, 0],\n",
    "            y=wav2vec_3d[mask, 1],\n",
    "            z=wav2vec_3d[mask, 2],\n",
    "            mode='markers',\n",
    "            name=word,\n",
    "            marker=dict(size=5, color=colors[word], opacity=0.7),\n",
    "            showlegend=True,\n",
    "            legendgroup=word\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Add Whisper traces\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=whisper_3d[mask, 0],\n",
    "            y=whisper_3d[mask, 1],\n",
    "            z=whisper_3d[mask, 2],\n",
    "            mode='markers',\n",
    "            name=word,\n",
    "            marker=dict(size=5, color=colors[word], opacity=0.7),\n",
    "            showlegend=False,  # Avoid duplicate legend\n",
    "            legendgroup=word\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1400,\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'\n",
    "    ),\n",
    "    scene2=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2108c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bdb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f8482",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_models = [\n",
    "    \"facebook/wav2vec2-base\",          # Pre-trained only (95M)\n",
    "    \"facebook/wav2vec2-base-960h\",     # Fine-tuned on LibriSpeech English (95M)\n",
    "    \"facebook/wav2vec2-large\",         # Pre-trained only (315M)\n",
    "    \"facebook/wav2vec2-large-960h\",    # Fine-tuned on LibriSpeech English (315M)\n",
    "    \"facebook/wav2vec2-large-960h-lv60\", # + pseudo-labeled English data (315M)\n",
    "    \"facebook/wav2vec2-large-960h-lv60-self\", # Self-training English (315M)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "whisper_models = [\n",
    "    # Multilingual versions\n",
    "    \"openai/whisper-tiny\",              # 39M params\n",
    "    \"openai/whisper-base\",              # 74M params\n",
    "    \"openai/whisper-small\",             # 244M params\n",
    "    \"openai/whisper-medium\",            # 769M params\n",
    "    \"openai/whisper-large\",             # 1550M params\n",
    "    \"openai/whisper-large-v2\",          # 1550M params (improved)\n",
    "    \"openai/whisper-large-v3\",          # 1550M params (latest)\n",
    "    \n",
    "    # English-only versions (faster, more accurate for English)\n",
    "    \"openai/whisper-tiny.en\",           # 39M params - English only\n",
    "    \"openai/whisper-base.en\",           # 74M params - English only\n",
    "    \"openai/whisper-small.en\",          # 244M params - English only\n",
    "    \"openai/whisper-medium.en\",         # 769M params - English only\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_commands_words = [\n",
    "    # Digits (10)\n",
    "    \"zero\", \"one\", \"two\", \"three\", \"four\", \n",
    "    \"five\", \"six\", \"seven\", \"eight\", \"nine\",\n",
    "    \n",
    "    # Core commands (10)\n",
    "    \"yes\", \"no\", \"up\", \"down\", \"left\", \n",
    "    \"right\", \"on\", \"off\", \"stop\", \"go\",\n",
    "    \n",
    "    # Additional commands (4)\n",
    "    \"backward\", \"forward\", \"follow\", \"learn\",\n",
    "    \n",
    "    # Auxiliary words (11)\n",
    "    \"bed\", \"bird\", \"cat\", \"dog\", \"happy\", \n",
    "    \"house\", \"marvin\", \"sheila\", \"tree\", \"wow\", \"visual\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263sc7akbk1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import DAC\n",
    "import sys\n",
    "sys.path.append('/home/amanag/videoEra/sound/wav2vec_vs_whisper/descript-audio-codec')\n",
    "import dac\n",
    "from audiotools import AudioSignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35t71lth7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DAC model\n",
    "model_path = dac.utils.download(model_type=\"16khz\")  # Using 16khz to match our dataset\n",
    "dac_model = dac.DAC.load(model_path)\n",
    "dac_model.eval()\n",
    "print(f\"DAC model loaded: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74zuk6up69j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update extract_embeddings to include DAC and use 10th position vector\n",
    "def extract_embeddings_all(audio_path):\n",
    "    \"\"\"Extract embeddings from all three models using 10th position vector\"\"\"\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Wav2Vec - use 10th position\n",
    "    inputs = wa2vec_processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = wa2vec_model(**inputs)\n",
    "        # Extract vector at position 10 instead of mean\n",
    "        # if outputs.last_hidden_state.shape[1] > 10:\n",
    "        wav2vec_emb = outputs.last_hidden_state[:, 10, :].squeeze().numpy()\n",
    "        # else:\n",
    "        #     wav2vec_emb = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    # Whisper - use 10th position\n",
    "    inputs = whisper_processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = whisper_model.encoder(**inputs)\n",
    "        # Extract vector at position 10 instead of mean\n",
    "        # if outputs.last_hidden_state.shape[1] > 10:\n",
    "        whisper_emb = outputs.last_hidden_state[:, 10, :].squeeze().numpy()\n",
    "        # else:\n",
    "        #     whisper_emb = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    # DAC - use 10th position\n",
    "    signal = AudioSignal(audio_path)\n",
    "    signal = signal.resample(16000)\n",
    "    with torch.no_grad():\n",
    "        audio_tensor = signal.audio_data\n",
    "        audio_tensor = dac_model.preprocess(audio_tensor, 16000)\n",
    "        z, codes, latents, _, _ = dac_model.encode(audio_tensor)\n",
    "        # Extract 1024-dim vector at position 10\n",
    "        # if z.shape[-1] > 10:\n",
    "        dac_emb = z[:, :, 10].squeeze().numpy()\n",
    "        # else:\n",
    "        #     dac_emb = z.mean(dim=-1).squeeze().numpy()\n",
    "    \n",
    "    return wav2vec_emb, whisper_emb, dac_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kac1ck4rr6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Use 4 workers for parallel processing\n",
    "n_jobs = 4\n",
    "print(f\"Using {n_jobs} workers for parallel processing\")\n",
    "\n",
    "# Collect all file paths first\n",
    "file_paths = []\n",
    "file_labels = []\n",
    "\n",
    "for word in test_words:\n",
    "    word_dir = os.path.join(dataset_path, word)\n",
    "    files = os.listdir(word_dir)[:50]  # Just 50 samples per word\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            path = os.path.join(word_dir, file)\n",
    "            file_paths.append(path)\n",
    "            file_labels.append(word)\n",
    "\n",
    "print(f\"Total files to process: {len(file_paths)}\")\n",
    "\n",
    "# Parallel extraction with progress bar (4 workers)\n",
    "results = Parallel(n_jobs=4)(\n",
    "    delayed(extract_embeddings_all)(path) \n",
    "    for path in tqdm(file_paths, desc=\"Extracting embeddings\")\n",
    ")\n",
    "\n",
    "# Unpack results\n",
    "wav2vec_embeddings_new = []\n",
    "whisper_embeddings_new = []\n",
    "dac_embeddings = []\n",
    "\n",
    "for wav2vec_emb, whisper_emb, dac_emb in results:\n",
    "    wav2vec_embeddings_new.append(wav2vec_emb)\n",
    "    whisper_embeddings_new.append(whisper_emb)\n",
    "    dac_embeddings.append(dac_emb)\n",
    "\n",
    "labels_new = file_labels\n",
    "\n",
    "print(f\"Extracted {len(wav2vec_embeddings_new)} Wav2Vec embeddings\")\n",
    "print(f\"Extracted {len(whisper_embeddings_new)} Whisper embeddings\")\n",
    "print(f\"Extracted {len(dac_embeddings)} DAC embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xcr03tuudy9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "wav2vec_embeddings_new = np.array(wav2vec_embeddings_new)\n",
    "whisper_embeddings_new = np.array(whisper_embeddings_new)\n",
    "dac_embeddings = np.array(dac_embeddings)\n",
    "\n",
    "print(f\"Wav2Vec embeddings shape: {wav2vec_embeddings_new.shape}\")\n",
    "print(f\"Whisper embeddings shape: {whisper_embeddings_new.shape}\")\n",
    "print(f\"DAC embeddings shape: {dac_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0zwzhzne8zs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization for all three models\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# t-SNE for all three models\n",
    "wav2vec_tsne = TSNE(n_components=2, random_state=42, metric='cosine', max_iter=10000, n_iter_without_progress=1000)\n",
    "wav2vec_2d_new = wav2vec_tsne.fit_transform(wav2vec_embeddings_new)\n",
    "\n",
    "whisper_tsne = TSNE(n_components=2, random_state=42, metric='cosine', max_iter=10000, n_iter_without_progress=1000)\n",
    "whisper_2d_new = whisper_tsne.fit_transform(whisper_embeddings_new)\n",
    "\n",
    "dac_tsne = TSNE(n_components=2, random_state=42, metric='cosine', max_iter=10000, n_iter_without_progress=1000)\n",
    "dac_2d = dac_tsne.fit_transform(dac_embeddings)\n",
    "\n",
    "# Create visualizations with all three models\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Color map for words\n",
    "colors = {'yes': 'red', 'no': 'blue', 'up': 'green', 'down': 'orange', 'left': 'purple'}\n",
    "\n",
    "# Plot Wav2Vec\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels_new]\n",
    "    ax1.scatter(wav2vec_2d_new[mask, 0], wav2vec_2d_new[mask, 1], \n",
    "                label=word, color=colors[word], alpha=0.6)\n",
    "ax1.set_title('Wav2Vec2 Embeddings (t-SNE, 10th position)')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot Whisper\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels_new]\n",
    "    ax2.scatter(whisper_2d_new[mask, 0], whisper_2d_new[mask, 1], \n",
    "                label=word, color=colors[word], alpha=0.6)\n",
    "ax2.set_title('Whisper Embeddings (t-SNE, 10th position)')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot DAC\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels_new]\n",
    "    ax3.scatter(dac_2d[mask, 0], dac_2d[mask, 1], \n",
    "                label=word, color=colors[word], alpha=0.6)\n",
    "ax3.set_title('DAC Embeddings (t-SNE, 10th position)')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dgy8vflxrse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA visualization for all three models\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# PCA for all three models with 3 components\n",
    "wav2vec_pca_new = PCA(n_components=3, random_state=42)\n",
    "wav2vec_3d_new = wav2vec_pca_new.fit_transform(wav2vec_embeddings_new)\n",
    "\n",
    "whisper_pca_new = PCA(n_components=3, random_state=42)\n",
    "whisper_3d_new = whisper_pca_new.fit_transform(whisper_embeddings_new)\n",
    "\n",
    "dac_pca = PCA(n_components=3, random_state=42)\n",
    "dac_3d = dac_pca.fit_transform(dac_embeddings)\n",
    "\n",
    "# Print variance explained for all models\n",
    "print(f\"Wav2Vec PCA variance explained: {wav2vec_pca_new.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"Whisper PCA variance explained: {whisper_pca_new.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"DAC PCA variance explained: {dac_pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Create 3D visualizations\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Color map for words\n",
    "colors = {'yes': 'red', 'no': 'blue', 'up': 'green', 'down': 'orange', 'left': 'purple'}\n",
    "\n",
    "# 3D Plot for Wav2Vec\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels_new]\n",
    "    ax1.scatter(wav2vec_3d_new[mask, 0], wav2vec_3d_new[mask, 1], wav2vec_3d_new[mask, 2],\n",
    "                label=word, color=colors[word], alpha=0.6, s=50)\n",
    "ax1.set_title('Wav2Vec2 Embeddings (3D PCA, 10th position)')\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_zlabel('PC3')\n",
    "ax1.legend()\n",
    "\n",
    "# 3D Plot for Whisper\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels_new]\n",
    "    ax2.scatter(whisper_3d_new[mask, 0], whisper_3d_new[mask, 1], whisper_3d_new[mask, 2],\n",
    "                label=word, color=colors[word], alpha=0.6, s=50)\n",
    "ax2.set_title('Whisper Embeddings (3D PCA, 10th position)')\n",
    "ax2.set_xlabel('PC1')\n",
    "ax2.set_ylabel('PC2')\n",
    "ax2.set_zlabel('PC3')\n",
    "ax2.legend()\n",
    "\n",
    "# 3D Plot for DAC\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels_new]\n",
    "    ax3.scatter(dac_3d[mask, 0], dac_3d[mask, 1], dac_3d[mask, 2],\n",
    "                label=word, color=colors[word], alpha=0.6, s=50)\n",
    "ax3.set_title('DAC Embeddings (3D PCA, 10th position)')\n",
    "ax3.set_xlabel('PC1')\n",
    "ax3.set_ylabel('PC2')\n",
    "ax3.set_zlabel('PC3')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edizmdw41u6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive 3D visualization with Plotly (all three models)\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create subplots with 3D scatter\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
    "    subplot_titles=('Wav2Vec2 (3D PCA, 10th pos)', 'Whisper (3D PCA, 10th pos)', 'DAC (3D PCA, 10th pos)')\n",
    ")\n",
    "\n",
    "# Add Wav2Vec traces\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels_new]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=wav2vec_3d_new[mask, 0],\n",
    "            y=wav2vec_3d_new[mask, 1],\n",
    "            z=wav2vec_3d_new[mask, 2],\n",
    "            mode='markers',\n",
    "            name=word,\n",
    "            marker=dict(size=5, color=colors[word], opacity=0.7),\n",
    "            showlegend=True,\n",
    "            legendgroup=word\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Add Whisper traces\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels_new]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=whisper_3d_new[mask, 0],\n",
    "            y=whisper_3d_new[mask, 1],\n",
    "            z=whisper_3d_new[mask, 2],\n",
    "            mode='markers',\n",
    "            name=word,\n",
    "            marker=dict(size=5, color=colors[word], opacity=0.7),\n",
    "            showlegend=False,\n",
    "            legendgroup=word\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Add DAC traces\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels_new]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=dac_3d[mask, 0],\n",
    "            y=dac_3d[mask, 1],\n",
    "            z=dac_3d[mask, 2],\n",
    "            mode='markers',\n",
    "            name=word,\n",
    "            marker=dict(size=5, color=colors[word], opacity=0.7),\n",
    "            showlegend=False,\n",
    "            legendgroup=word\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1800,\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'\n",
    "    ),\n",
    "    scene2=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'\n",
    "    ),\n",
    "    scene3=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f808017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "def extract_cnn_features(audio_path, model_type='wav2vec'):\n",
    "    \"\"\"Extract CNN features (transformer input) from audio\"\"\"\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    if model_type == 'wav2vec':\n",
    "        inputs = wa2vec_processor(audio, sampling_rate=16000, return_tensors=\"pt\")  # Fixed: wa2vec_processor\n",
    "        with torch.no_grad():\n",
    "            # Get CNN features (before transformer)\n",
    "            extract_features = wa2vec_model.feature_extractor(inputs.input_values)  # Fixed: wa2vec_model\n",
    "            features = extract_features.transpose(1, 2)  # Shape: (batch, time, features)\n",
    "            # Average pool across time dimension\n",
    "            features = features.mean(dim=1).squeeze().numpy()\n",
    "            \n",
    "    elif model_type == 'whisper':\n",
    "        inputs = whisper_processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            # Get mel spectrogram\n",
    "            mel = inputs.input_features\n",
    "            # Pass through conv layers (before transformer)\n",
    "            conv1 = whisper_model.encoder.conv1(mel)\n",
    "            conv2 = whisper_model.encoder.conv2(conv1)\n",
    "            # Reshape and add positional embeddings\n",
    "            features = conv2.permute(0, 2, 1)  # Shape: (batch, time, features)\n",
    "            # Average pool across time dimension\n",
    "            features = features.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract CNN features for all samples\n",
    "wav2vec_cnn_features = []\n",
    "whisper_cnn_features = []\n",
    "labels_cnn = []\n",
    "\n",
    "print(\"Extracting CNN features (transformer inputs)...\")\n",
    "for word in test_words:\n",
    "    word_dir = os.path.join(dataset_path, word)\n",
    "    files = os.listdir(word_dir)[:50]\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            path = os.path.join(word_dir, file)\n",
    "            \n",
    "            wav2vec_feat = extract_cnn_features(path, 'wav2vec')\n",
    "            whisper_feat = extract_cnn_features(path, 'whisper')\n",
    "            \n",
    "            wav2vec_cnn_features.append(wav2vec_feat)\n",
    "            whisper_cnn_features.append(whisper_feat)\n",
    "            labels_cnn.append(word)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "wav2vec_cnn_features = np.array(wav2vec_cnn_features)\n",
    "whisper_cnn_features = np.array(whisper_cnn_features)\n",
    "\n",
    "print(f\"Wav2Vec CNN features shape: {wav2vec_cnn_features.shape}\")\n",
    "print(f\"Whisper CNN features shape: {whisper_cnn_features.shape}\")\n",
    "\n",
    "# Apply PCA for visualization\n",
    "wav2vec_cnn_pca = PCA(n_components=2, random_state=42)\n",
    "wav2vec_cnn_2d = wav2vec_cnn_pca.fit_transform(wav2vec_cnn_features)\n",
    "\n",
    "whisper_cnn_pca = PCA(n_components=2, random_state=42)\n",
    "whisper_cnn_2d = whisper_cnn_pca.fit_transform(whisper_cnn_features)\n",
    "\n",
    "# Visualize\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "colors = {'yes': 'red', 'no': 'blue', 'up': 'green', 'down': 'orange', 'left': 'purple'}\n",
    "\n",
    "# Plot Wav2Vec CNN features\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels_cnn]\n",
    "    ax1.scatter(wav2vec_cnn_2d[mask, 0], wav2vec_cnn_2d[mask, 1], \n",
    "                label=word, color=colors[word], alpha=0.6)\n",
    "ax1.set_title('Wav2Vec2 CNN Features (Pre-Transformer)')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot Whisper CNN features\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels_cnn]\n",
    "    ax2.scatter(whisper_cnn_2d[mask, 0], whisper_cnn_2d[mask, 1], \n",
    "                label=word, color=colors[word], alpha=0.6)\n",
    "ax2.set_title('Whisper CNN Features (Pre-Transformer)')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot final embeddings for comparison\n",
    "wav2vec_pca_final = PCA(n_components=2, random_state=42)\n",
    "wav2vec_final_2d = wav2vec_pca_final.fit_transform(wav2vec_embeddings)\n",
    "\n",
    "whisper_pca_final = PCA(n_components=2, random_state=42)\n",
    "whisper_final_2d = whisper_pca_final.fit_transform(whisper_embeddings)\n",
    "\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels]\n",
    "    ax3.scatter(wav2vec_final_2d[mask, 0], wav2vec_final_2d[mask, 1], \n",
    "                label=word, color=colors[word], alpha=0.6)\n",
    "ax3.set_title('Wav2Vec2 Final Embeddings (Post-Transformer)')\n",
    "ax3.legend()\n",
    "\n",
    "for word in test_words:\n",
    "    mask = [l == word for l in labels]\n",
    "    ax4.scatter(whisper_final_2d[mask, 0], whisper_final_2d[mask, 1], \n",
    "                label=word, color=colors[word], alpha=0.6)\n",
    "ax4.set_title('Whisper Final Embeddings (Post-Transformer)')\n",
    "ax4.legend()\n",
    "\n",
    "plt.suptitle('CNN Features vs Final Embeddings Comparison', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print variance explained\n",
    "print(f\"\\nVariance explained by 2 PCs:\")\n",
    "print(f\"Wav2Vec CNN: {wav2vec_cnn_pca.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"Whisper CNN: {whisper_cnn_pca.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"Wav2Vec Final: {wav2vec_pca_final.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"Whisper Final: {whisper_pca_final.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3676140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer block details only\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRANSFORMER ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Wav2Vec2 Transformer Details\n",
    "print(\"\\n--- WAV2VEC2 TRANSFORMER ENCODER ---\")\n",
    "print(f\"Number of layers: {len(wa2vec_model.encoder.layers)}\")\n",
    "print(f\"Hidden size: {wa2vec_model.config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {wa2vec_model.config.num_attention_heads}\")\n",
    "print(f\"Intermediate size (FFN): {wa2vec_model.config.intermediate_size}\")\n",
    "print(f\"Hidden dropout: {wa2vec_model.config.hidden_dropout}\")\n",
    "print(f\"Attention dropout: {wa2vec_model.config.attention_dropout}\")\n",
    "\n",
    "# Show one transformer layer structure\n",
    "print(\"\\nSample Transformer Layer Structure:\")\n",
    "print(wa2vec_model.encoder.layers[0])\n",
    "\n",
    "# Whisper Transformer Details\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n--- WHISPER TRANSFORMER ENCODER ---\")\n",
    "print(f\"Number of layers: {len(whisper_model.encoder.layers)}\")\n",
    "print(f\"Hidden size: {whisper_model.config.d_model}\")\n",
    "print(f\"Number of attention heads: {whisper_model.config.encoder_attention_heads}\")\n",
    "print(f\"FFN dimension: {whisper_model.config.encoder_ffn_dim}\")\n",
    "print(f\"Dropout: {whisper_model.config.dropout}\")\n",
    "print(f\"Attention dropout: {whisper_model.config.attention_dropout}\")\n",
    "\n",
    "# Show one transformer layer structure\n",
    "print(\"\\nSample Transformer Layer Structure:\")\n",
    "print(whisper_model.encoder.layers[0])\n",
    "\n",
    "# Quick comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIDE-BY-SIDE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Property':<30} | {'Wav2Vec2':<20} | {'Whisper':<20}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"{'Transformer Layers':<30} | {len(wa2vec_model.encoder.layers):<20} | {len(whisper_model.encoder.layers):<20}\")\n",
    "print(f\"{'Hidden Dimension':<30} | {wa2vec_model.config.hidden_size:<20} | {whisper_model.config.d_model:<20}\")\n",
    "print(f\"{'Attention Heads':<30} | {wa2vec_model.config.num_attention_heads:<20} | {whisper_model.config.encoder_attention_heads:<20}\")\n",
    "print(f\"{'FFN Dimension':<30} | {wa2vec_model.config.intermediate_size:<20} | {whisper_model.config.encoder_ffn_dim:<20}\")\n",
    "print(f\"{'Head Dimension':<30} | {wa2vec_model.config.hidden_size // wa2vec_model.config.num_attention_heads:<20} | {whisper_model.config.d_model // whisper_model.config.encoder_attention_heads:<20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b179d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
