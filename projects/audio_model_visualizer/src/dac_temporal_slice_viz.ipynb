{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAC Temporal Slice Visualization\n",
    "\n",
    "This notebook explores DAC embeddings at **specific time steps** without temporal averaging.\n",
    "\n",
    "## Goal:\n",
    "- Extract 12,288D concatenated projections (12 codebooks √ó 1024D each) at a **single time index**\n",
    "- Compare clustering across different time positions (beginning, middle, end)\n",
    "- Understand how temporal position affects word discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Import our utilities\n",
    "from dac_utils import DACProcessor, SpeechCommandsLoader\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize DAC and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DAC processor\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dac_processor = DACProcessor(model_type=\"16khz\", device=device)\n",
    "\n",
    "# Load dataset\n",
    "loader = SpeechCommandsLoader()\n",
    "words = ['zero', 'one', 'two', 'yes', 'no']\n",
    "samples_per_word = 10\n",
    "\n",
    "file_paths, file_labels = loader.load_word_samples(words, samples_per_word=samples_per_word)\n",
    "print(f\"\\nLoaded {len(file_paths)} audio files from {len(words)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Embeddings at Specific Time Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_slice(dac_processor, audio_path, time_index):\n",
    "    \"\"\"\n",
    "    Extract 12,288D embedding at a specific time index.\n",
    "    \n",
    "    Args:\n",
    "        dac_processor: DACProcessor instance\n",
    "        audio_path: Path to audio file\n",
    "        time_index: Time step to extract (0 to T-1)\n",
    "    \n",
    "    Returns:\n",
    "        12,288D numpy array representing concatenated projections at time_index\n",
    "    \"\"\"\n",
    "    # Encode audio\n",
    "    encoded = dac_processor.encode_audio(audio_path)\n",
    "    codes = encoded['codes'][0]  # [n_codebooks, time]\n",
    "    \n",
    "    N = codes.shape[0]  # Number of codebooks (12)\n",
    "    T = codes.shape[1]  # Number of time steps\n",
    "    \n",
    "    # Validate time index\n",
    "    if time_index >= T:\n",
    "        raise ValueError(f\"time_index {time_index} exceeds sequence length {T}\")\n",
    "    \n",
    "    # Extract projection at specific time index for each codebook\n",
    "    codebook_projections = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        quantizer = dac_processor.model.quantizer.quantizers[i]\n",
    "        \n",
    "        # Get indices for this codebook [time]\n",
    "        indices = codes[i:i+1, :].to(dac_processor.device)  # [1, time]\n",
    "        \n",
    "        # Get 8D embeddings [1, time, 8]\n",
    "        z_e = quantizer.embed_code(indices)\n",
    "        \n",
    "        # Project to 1024D [1, 1024, time]\n",
    "        z_q = quantizer.out_proj(z_e.transpose(1, 2))\n",
    "        \n",
    "        # Extract specific time index [1, 1024]\n",
    "        z_q_slice = z_q[:, :, time_index]\n",
    "        \n",
    "        codebook_projections.append(z_q_slice)\n",
    "    \n",
    "    # Concatenate all codebooks [1, 12288]\n",
    "    concatenated = torch.cat(codebook_projections, dim=1)\n",
    "    \n",
    "    return concatenated.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "# Test extraction\n",
    "test_embedding = extract_temporal_slice(dac_processor, file_paths[0], time_index=10)\n",
    "print(f\"\\nExtracted embedding shape: {test_embedding.shape}\")\n",
    "print(f\"Embedding stats: mean={test_embedding.mean():.4f}, std={test_embedding.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inspect Temporal Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sequence lengths across samples\n",
    "sequence_lengths = []\n",
    "for file_path in file_paths[:5]:  # Check first 5\n",
    "    encoded = dac_processor.encode_audio(file_path)\n",
    "    T = encoded['codes'].shape[2]\n",
    "    sequence_lengths.append(T)\n",
    "\n",
    "print(f\"Sample sequence lengths: {sequence_lengths}\")\n",
    "print(f\"Min: {min(sequence_lengths)}, Max: {max(sequence_lengths)}\")\n",
    "print(f\"\\n‚Üí We'll use time indices that fit all samples (0 to {min(sequence_lengths)-1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize at Different Time Positions\n",
    "\n",
    "We'll compare 3 temporal positions:\n",
    "- **Early** (time_index=5): Beginning of the word\n",
    "- **Middle** (time_index=25): Core phonetic content\n",
    "- **Late** (time_index=45): End of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time indices to explore\n",
    "time_positions = {\n",
    "    'Early (t=5)': 5,\n",
    "    'Middle (t=25)': 25,\n",
    "    'Late (t=45)': 45\n",
    "}\n",
    "\n",
    "# Extract embeddings for each time position\n",
    "results = {}\n",
    "\n",
    "for position_name, time_idx in time_positions.items():\n",
    "    print(f\"\\nExtracting embeddings at {position_name}...\")\n",
    "    \n",
    "    embeddings = []\n",
    "    valid_labels = []\n",
    "    \n",
    "    for file_path, label in zip(file_paths, file_labels):\n",
    "        try:\n",
    "            emb = extract_temporal_slice(dac_processor, file_path, time_idx)\n",
    "            embeddings.append(emb)\n",
    "            valid_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipped {file_path}: {e}\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    print(f\"  Extracted {len(embeddings)} embeddings, shape: {embeddings.shape}\")\n",
    "    \n",
    "    results[position_name] = {\n",
    "        'embeddings': embeddings,\n",
    "        'labels': valid_labels,\n",
    "        'time_index': time_idx\n",
    "    }\n",
    "\n",
    "print(\"\\n‚úÖ Extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: PCA Visualization Across Time Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color map\n",
    "color_map = {word: px.colors.qualitative.Plotly[i] for i, word in enumerate(words)}\n",
    "\n",
    "# PCA for each time position\n",
    "fig = go.Figure()\n",
    "\n",
    "for position_name, data in results.items():\n",
    "    embeddings = data['embeddings']\n",
    "    labels = data['labels']\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(embeddings)\n",
    "    \n",
    "    variance_explained = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"{position_name}: PCA variance explained = {variance_explained:.2%}\")\n",
    "    \n",
    "    # Store for metrics\n",
    "    data['pca_result'] = pca_result\n",
    "    data['pca_variance'] = variance_explained\n",
    "\n",
    "# Create subplots for comparison\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=[f\"{name}<br>Var: {data['pca_variance']:.1%}\" \n",
    "                    for name, data in results.items()]\n",
    ")\n",
    "\n",
    "for idx, (position_name, data) in enumerate(results.items(), 1):\n",
    "    pca_result = data['pca_result']\n",
    "    labels = data['labels']\n",
    "    \n",
    "    for word in words:\n",
    "        mask = np.array([label == word for label in labels])\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=pca_result[mask, 0],\n",
    "                y=pca_result[mask, 1],\n",
    "                mode='markers',\n",
    "                name=word,\n",
    "                marker=dict(size=8, color=color_map[word], opacity=0.7),\n",
    "                legendgroup=word,\n",
    "                showlegend=(idx == 1)  # Only show legend once\n",
    "            ),\n",
    "            row=1, col=idx\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='PCA: DAC Embeddings at Different Time Positions (12,288D)',\n",
    "    height=400,\n",
    "    width=1400\n",
    ")\n",
    "\n",
    "fig.write_html('dac_temporal_slice_pca.html')\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nSaved: dac_temporal_slice_pca.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: t-SNE Visualization Across Time Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE for each time position\n",
    "perplexity = min(30, len(file_paths) - 1)\n",
    "\n",
    "for position_name, data in results.items():\n",
    "    embeddings = data['embeddings']\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "    tsne_result = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Store for metrics\n",
    "    data['tsne_result'] = tsne_result\n",
    "    print(f\"{position_name}: t-SNE completed\")\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=[name for name in results.keys()]\n",
    ")\n",
    "\n",
    "for idx, (position_name, data) in enumerate(results.items(), 1):\n",
    "    tsne_result = data['tsne_result']\n",
    "    labels = data['labels']\n",
    "    \n",
    "    for word in words:\n",
    "        mask = np.array([label == word for label in labels])\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=tsne_result[mask, 0],\n",
    "                y=tsne_result[mask, 1],\n",
    "                mode='markers',\n",
    "                name=word,\n",
    "                marker=dict(size=8, color=color_map[word], opacity=0.7),\n",
    "                legendgroup=word,\n",
    "                showlegend=(idx == 1)\n",
    "            ),\n",
    "            row=1, col=idx\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='t-SNE: DAC Embeddings at Different Time Positions (12,288D)',\n",
    "    height=400,\n",
    "    width=1400\n",
    ")\n",
    "\n",
    "fig.write_html('dac_temporal_slice_tsne.html')\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nSaved: dac_temporal_slice_tsne.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Clustering Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numeric\n",
    "label_to_idx = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTERING METRICS: TEMPORAL SLICE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "metrics_table = []\n",
    "\n",
    "for position_name, data in results.items():\n",
    "    labels = data['labels']\n",
    "    numeric_labels = np.array([label_to_idx[label] for label in labels])\n",
    "    \n",
    "    # PCA metrics\n",
    "    pca_sil = silhouette_score(data['pca_result'], numeric_labels)\n",
    "    pca_db = davies_bouldin_score(data['pca_result'], numeric_labels)\n",
    "    \n",
    "    # t-SNE metrics\n",
    "    tsne_sil = silhouette_score(data['tsne_result'], numeric_labels)\n",
    "    tsne_db = davies_bouldin_score(data['tsne_result'], numeric_labels)\n",
    "    \n",
    "    print(f\"\\n{position_name}:\")\n",
    "    print(f\"  PCA:   Silhouette = {pca_sil:+.4f}  |  Davies-Bouldin = {pca_db:.4f}\")\n",
    "    print(f\"  t-SNE: Silhouette = {tsne_sil:+.4f}  |  Davies-Bouldin = {tsne_db:.4f}\")\n",
    "    \n",
    "    metrics_table.append({\n",
    "        'position': position_name,\n",
    "        'time_idx': data['time_index'],\n",
    "        'pca_sil': pca_sil,\n",
    "        'pca_db': pca_db,\n",
    "        'tsne_sil': tsne_sil,\n",
    "        'tsne_db': tsne_db\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find best position\n",
    "best_pca = max(metrics_table, key=lambda x: x['pca_sil'])\n",
    "best_tsne = max(metrics_table, key=lambda x: x['tsne_sil'])\n",
    "\n",
    "print(f\"\\nüèÜ Best PCA clustering: {best_pca['position']} (Silhouette: {best_pca['pca_sil']:+.4f})\")\n",
    "print(f\"üèÜ Best t-SNE clustering: {best_tsne['position']} (Silhouette: {best_tsne['tsne_sil']:+.4f})\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "if max(m['pca_sil'] for m in metrics_table) > 0.1:\n",
    "    print(\"  ‚úÖ Specific time positions show better clustering!\")\n",
    "    print(\"  ‚Üí Certain phonetic moments are more discriminative\")\n",
    "else:\n",
    "    print(\"  ‚ùå Even at specific time slices, clustering remains poor\")\n",
    "    print(\"  ‚Üí Single time steps lack sufficient context for word discrimination\")\n",
    "    print(\"  ‚Üí Need to preserve full temporal sequences (RNN/LSTM/Transformer)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Custom Time Index Exploration\n",
    "\n",
    "Use this cell to explore any specific time index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your desired time index here\n",
    "CUSTOM_TIME_INDEX = 10  # Change this value (0 to 49)\n",
    "\n",
    "print(f\"Extracting embeddings at time index: {CUSTOM_TIME_INDEX}\")\n",
    "\n",
    "custom_embeddings = []\n",
    "custom_labels = []\n",
    "\n",
    "for file_path, label in zip(file_paths, file_labels):\n",
    "    try:\n",
    "        emb = extract_temporal_slice(dac_processor, file_path, CUSTOM_TIME_INDEX)\n",
    "        custom_embeddings.append(emb)\n",
    "        custom_labels.append(label)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped: {e}\")\n",
    "\n",
    "custom_embeddings = np.array(custom_embeddings)\n",
    "print(f\"Shape: {custom_embeddings.shape}\")\n",
    "\n",
    "# Quick PCA visualization\n",
    "pca_custom = PCA(n_components=2)\n",
    "pca_custom_result = pca_custom.fit_transform(custom_embeddings)\n",
    "\n",
    "fig = go.Figure()\n",
    "for word in words:\n",
    "    mask = np.array([label == word for label in custom_labels])\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pca_custom_result[mask, 0],\n",
    "        y=pca_custom_result[mask, 1],\n",
    "        mode='markers',\n",
    "        name=word,\n",
    "        marker=dict(size=10, color=color_map[word], opacity=0.7)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'PCA at Time Index {CUSTOM_TIME_INDEX}',\n",
    "    xaxis_title='PC1',\n",
    "    yaxis_title='PC2',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Metrics\n",
    "numeric_custom = np.array([label_to_idx[label] for label in custom_labels])\n",
    "sil_custom = silhouette_score(pca_custom_result, numeric_custom)\n",
    "db_custom = davies_bouldin_score(pca_custom_result, numeric_custom)\n",
    "\n",
    "print(f\"\\nMetrics for time index {CUSTOM_TIME_INDEX}:\")\n",
    "print(f\"  Silhouette: {sil_custom:+.4f}\")\n",
    "print(f\"  Davies-Bouldin: {db_custom:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook explored DAC embeddings **without temporal averaging**:\n",
    "1. ‚úÖ Extracted 12,288D concatenated projections at specific time indices\n",
    "2. ‚úÖ Compared clustering quality across Early/Middle/Late positions\n",
    "3. ‚úÖ Provided custom time index exploration capability\n",
    "\n",
    "**Key Finding**: Single time slices likely show poor clustering because:\n",
    "- Words are temporal sequences, not static snapshots\n",
    "- Phonetic information unfolds over time\n",
    "- DAC optimized for compression, not phonetic discrimination\n",
    "\n",
    "**Conclusion**: To use DAC for speech tasks, need sequence models that preserve full temporal context (RNN/LSTM/Transformer), not single-frame or averaged representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
